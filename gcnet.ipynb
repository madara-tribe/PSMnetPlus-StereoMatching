{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89e30a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class CostNet(nn.Module):\n",
    "\n",
    "    def __init__(self, k=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = CNN(k=k)\n",
    "        self.spp = SPP()\n",
    "        self.fusion = nn.Sequential(\n",
    "                Conv2dBn(in_channels=320, out_channels=128, kernel_size=3, stride=1, padding=1, use_relu=True),\n",
    "                nn.Conv2d(in_channels=128, out_channels=32, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        conv2_out, conv4_out = self.cnn(inputs)           # [B, 64, 1/4H, 1/4W], [B, 128, 1/4H, 1/4W]\n",
    "\n",
    "        spp_out = self.spp(conv4_out)                    # [B, 128, 1/4H, 1/4W]\n",
    "        print(\"costnet2\", conv2_out.shape, conv4_out.shape, spp_out.shape)\n",
    "        \n",
    "        out = torch.cat([conv2_out, conv4_out, spp_out], dim=1)  # [B, 320, 1/4H, 1/4W]\n",
    "        print(\"costnet3\",out.shape)\n",
    "        out = self.fusion(out)                            # [B, 32, 1/4H, 1/4W]\n",
    "        print(\"fusion out\", out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SPP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch1 = self.__make_branch(kernel_size=64, stride=64)\n",
    "        self.branch2 = self.__make_branch(kernel_size=32, stride=32)\n",
    "        self.branch3 = self.__make_branch(kernel_size=16, stride=16)\n",
    "        self.branch4 = self.__make_branch(kernel_size=8, stride=8)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        out_size = inputs.size(2), inputs.size(3)\n",
    "        branch1_out = F.upsample(self.branch1(inputs), size=out_size, mode='bilinear')  # [B, 32, 1/4H, 1/4W]\n",
    "        # print('branch1_out')\n",
    "        # print(branch1_out[0, 0, :3, :3])\n",
    "        branch2_out = F.upsample(self.branch2(inputs), size=out_size, mode='bilinear')  # [B, 32, 1/4H, 1/4W]\n",
    "        branch3_out = F.upsample(self.branch3(inputs), size=out_size, mode='bilinear')  # [B, 32, 1/4H, 1/4W]\n",
    "        branch4_out = F.upsample(self.branch4(inputs), size=out_size, mode='bilinear')  # [B, 32, 1/4H, 1/4W]\n",
    "        out = torch.cat([branch4_out, branch3_out, branch2_out, branch1_out], dim=1)  # [B, 128, 1/4H, 1/4W]\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def __make_branch(kernel_size, stride):\n",
    "        branch = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size, stride),\n",
    "                Conv2dBn(in_channels=128, out_channels=32, kernel_size=3, stride=1, padding=1, use_relu=True)  # kernel size maybe 1\n",
    "            )\n",
    "        return branch\n",
    "\n",
    "class Squeeze_excitation_layer(nn.Module):\n",
    "    def __init__(self, filters, se_ratio=4):\n",
    "        super(Squeeze_excitation_layer, self).__init__()\n",
    "        reduction = filters // se_ratio\n",
    "        self.se = nn.Sequential(nn.Conv2d(filters, reduction, kernel_size=1, bias=True),\n",
    "                                nn.SiLU(),\n",
    "                                nn.Conv2d(reduction, filters, kernel_size=1, bias=True),\n",
    "                                nn.Sigmoid())\n",
    "    def forward(self, inputs):\n",
    "        x = self.se(inputs)\n",
    "        return torch.multiply(inputs, x)\n",
    "\n",
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size = 3, padding = 1, bias=False):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, groups=nin, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "class MBConv2d_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k=1):\n",
    "        super(MBConv2d_block, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(nn.Conv2d(in_channels, out_channels * k, kernel_size=(1, 1), stride=(1, 1), padding=\"valid\", bias=False),\n",
    "               nn.BatchNorm2d(out_channels * k),\n",
    "               nn.SiLU(),\n",
    "               depthwise_separable_conv(out_channels * k, out_channels * k, kernel_size = 3, padding =\"same\", bias=False),\n",
    "               nn.BatchNorm2d(out_channels * k),\n",
    "               nn.SiLU(),\n",
    "               Squeeze_excitation_layer(filters=out_channels * k, se_ratio=4),\n",
    "               nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1), stride=(1, 1), padding=\"valid\", bias=False),\n",
    "               nn.BatchNorm2d(out_channels * k),\n",
    "               nn.Dropout(p=0.2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.net(inputs)\n",
    "        return torch.add(inputs, x)\n",
    "  \n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, k=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv0 = nn.Sequential(\n",
    "                Conv2dBn(in_channels=3, out_channels=k, kernel_size=3, stride=2, padding=1, use_relu=True),  # downsample\n",
    "                Conv2dBn(in_channels=k, out_channels=k, kernel_size=3, stride=1, padding=1, use_relu=True),\n",
    "                Conv2dBn(in_channels=k, out_channels=k, kernel_size=3, stride=1, padding=1, use_relu=True)\n",
    "            )\n",
    "        self.mbconv0 = MBConv2d_block(in_channels=k, out_channels=k, k=1)\n",
    "        self.conv1 = StackedBlocks(n_blocks=3, in_channels=k, out_channels=k, kernel_size=3, stride=1, padding=1, dilation=1)\n",
    "        self.mbconv1 = MBConv2d_block(in_channels=k, out_channels=k, k=1)\n",
    "        self.conv2 = StackedBlocks(n_blocks=3, in_channels=k, out_channels=k*2, kernel_size=3, stride=2, padding=1, dilation=1)  # downsample\n",
    "        self.mbconv2 = MBConv2d_block(in_channels=k*2, out_channels=k*2, k=1)\n",
    "        self.conv3 = StackedBlocks(n_blocks=3, in_channels=k*2, out_channels=k*4, kernel_size=3, stride=1, padding=2, dilation=2)  # dilated\n",
    "        self.mbconv3 = MBConv2d_block(in_channels=k*4, out_channels=k*4, k=1)\n",
    "        self.conv4 = StackedBlocks(n_blocks=3, in_channels=k*4, out_channels=k*4, kernel_size=3, stride=1, padding=4, dilation=4)  # dilated\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        conv0_out = self.mbconv0(self.conv0(inputs))\n",
    "        conv1_out = self.mbconv1(self.conv1(conv0_out))  # [B, 32, 1/2H, 1/2W]\n",
    "        conv2_out = self.mbconv2(self.conv2(conv1_out))  # [B, 64, 1/4H, 1/4W]\n",
    "        conv3_out = self.mbconv3(self.conv3(conv2_out))  # [B, 128, 1/4H, 1/4W]\n",
    "        conv4_out = self.conv4(conv3_out)  # [B, 128, 1/4H, 1/4W]\n",
    "        print(\"inp, c0 c1\", inputs.shape, conv0_out.shape, conv1_out.shape)\n",
    "        print(\"c2 c3 c4\", conv2_out.shape, conv3_out.shape, conv4_out.shape)\n",
    "        return conv2_out, conv4_out\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, k=64):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Sequential(\n",
    "                Conv2dBn(in_channels=3, out_channels=k, kernel_size=3, stride=2, padding=1, use_relu=True),  # downsample\n",
    "                Conv2dBn(in_channels=k, out_channels=k, kernel_size=3, stride=1, padding=1, use_relu=True),\n",
    "                Conv2dBn(in_channels=k, out_channels=k, kernel_size=3, stride=1, padding=1, use_relu=True),\n",
    "                Squeeze_excitation_layer(filters=k, se_ratio=4)\n",
    "                )\n",
    "        self.conv1 = nn.Sequential(\n",
    "                StackedBlocks(n_blocks=3, in_channels=k, out_channels=k*2, kernel_size=3, stride=1, padding=1, dilation=1),\n",
    "                MBConv2d_block(in_channels=k*2, out_channels=k*2, k=1)\n",
    "                )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                StackedBlocks(n_blocks=3, in_channels=k*2, out_channels=k*4, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "                MBConv2d_block(in_channels=k*4, out_channels=k*4, k=1)\n",
    "                )\n",
    "        #self.conv2 = StackedBlocks(n_blocks=16, in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1, dilation=1)  # downsample\n",
    "        self.conv3 = nn.Sequential(\n",
    "                StackedBlocks(n_blocks=3, in_channels=k*4, out_channels=k*8, kernel_size=3, stride=1, padding=2, dilation=2),  # dilated\n",
    "                MBConv2d_block(in_channels=k*8, out_channels=k*8, k=1),\n",
    "                MBConv2d_block(in_channels=k*8, out_channels=k*8, k=1)\n",
    "                )\n",
    "    def forward(self, inputs): # inputs is [4, 3, 384, 1280]\n",
    "        conv0_out = self.conv0(inputs) # [4, 32, 192, 640]\n",
    "        conv1_out = self.conv1(conv0_out)  # [B, 32, 1/2H, 1/2W] # [4, 32, 192, 640]\n",
    "        conv2_out = self.conv2(conv1_out)  # [B, 64, 1/4H, 1/4W] # [4, 64, 96, 320]\n",
    "        conv3_out = self.conv3(conv2_out)  # [B, 128, 1/4H, 1/4W] # [4, 128, 96, 320]\n",
    "        print(\"inp, c0 c1\", inputs.shape, conv0_out.shape, conv1_out.shape)\n",
    "        print(\"c2 c3 c4\", conv2_out.shape, conv3_out.shape)\n",
    "        return conv2_out, conv3_out\n",
    "\"\"\"\n",
    "\n",
    "class StackedBlocks(nn.Module):\n",
    "\n",
    "    def __init__(self, n_blocks, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super().__init__()\n",
    "\n",
    "        if stride == 1 and in_channels == out_channels:\n",
    "            downsample = False\n",
    "        else:\n",
    "            downsample = True\n",
    "        net = [ResidualBlock(in_channels, out_channels, kernel_size, stride, padding, dilation, downsample)]\n",
    "\n",
    "        for i in range(n_blocks - 1):\n",
    "            net.append(ResidualBlock(out_channels, out_channels, kernel_size, 1, padding, dilation, downsample=False))\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.net(inputs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, downsample=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "                Conv2dBn(in_channels, out_channels, kernel_size, stride, padding, dilation, use_relu=True),\n",
    "                Conv2dBn(out_channels, out_channels, kernel_size, 1, padding, dilation, use_relu=False)\n",
    "            )\n",
    "\n",
    "        self.downsample = None\n",
    "        if downsample:\n",
    "            self.downsample = Conv2dBn(in_channels, out_channels, 1, stride, use_relu=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.net(inputs)\n",
    "        if self.downsample:\n",
    "            inputs = self.downsample(inputs)\n",
    "        out = out + inputs\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Conv3dBn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, use_relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        net = [nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=False),\n",
    "               nn.BatchNorm3d(out_channels)]\n",
    "        if use_relu:\n",
    "            net.append(nn.SiLU(inplace=True))\n",
    "\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.net(inputs)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    " \n",
    "class Conv2dBn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, use_relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        net = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=False),\n",
    "               nn.BatchNorm2d(out_channels)]\n",
    "        if use_relu:\n",
    "            net.append(nn.SiLU(inplace=True))\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.net(inputs)\n",
    "        return out\n",
    "  \n",
    "class StackedHourglass(nn.Module):\n",
    "    '''\n",
    "    inputs --- [B, 64, 1/4D, 1/4H, 1/4W]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_disp):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv0 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True),\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True)\n",
    "        )\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True),\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=False)\n",
    "        )\n",
    "        self.hourglass1 = Hourglass()\n",
    "        self.hourglass2 = Hourglass()\n",
    "        self.hourglass3 = Hourglass()\n",
    "\n",
    "        self.out1 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True),\n",
    "            nn.Conv3d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        )\n",
    "        self.out2 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True),\n",
    "            nn.Conv3d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        )\n",
    "        self.out3 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True),\n",
    "            nn.Conv3d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.regression = DisparityRegression(max_disp)\n",
    "\n",
    "    def forward(self, inputs, out_size):\n",
    "\n",
    "        conv0_out = self.conv0(inputs)     # [B, 32, 1/4D, 1/4H, 1/4W]\n",
    "        conv1_out = self.conv1(conv0_out)\n",
    "        conv1_out = conv0_out + conv1_out  # [B, 32, 1/4D, 1/4H, 1/4W]\n",
    "\n",
    "        hourglass1_out1, hourglass1_out3, hourglass1_out4 = self.hourglass1(conv1_out, scale1=None, scale2=None, scale3=conv1_out)\n",
    "        hourglass2_out1, hourglass2_out3, hourglass2_out4 = self.hourglass2(hourglass1_out4, scale1=hourglass1_out3, scale2=hourglass1_out1, scale3=conv1_out)\n",
    "        hourglass3_out1, hourglass3_out3, hourglass3_out4 = self.hourglass3(hourglass2_out4, scale1=hourglass2_out3, scale2=hourglass1_out1, scale3=conv1_out)\n",
    "\n",
    "        out1 = self.out1(hourglass1_out4)  # [B, 1, 1/4D, 1/4H, 1/4W]\n",
    "        out2 = self.out2(hourglass2_out4) + out1\n",
    "        out3 = self.out3(hourglass3_out4) + out2\n",
    "\n",
    "        cost1 = F.upsample(out1, size=out_size, mode='trilinear').squeeze(dim=1)  # [B, D, H, W]\n",
    "        cost2 = F.upsample(out2, size=out_size, mode='trilinear').squeeze(dim=1)  # [B, D, H, W]\n",
    "        cost3 = F.upsample(out3, size=out_size, mode='trilinear').squeeze(dim=1)  # [B, D, H, W]\n",
    "\n",
    "        prob1 = F.softmax(-cost1, dim=1)  # [B, D, H, W]\n",
    "        prob2 = F.softmax(-cost2, dim=1)\n",
    "        prob3 = F.softmax(-cost3, dim=1)\n",
    "\n",
    "        disp1 = self.regression(prob1)\n",
    "        disp2 = self.regression(prob2)\n",
    "        disp3 = self.regression(prob3)\n",
    "\n",
    "        return disp1, disp2, disp3\n",
    "\n",
    "\n",
    "class DisparityRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, max_disp):\n",
    "        super().__init__()\n",
    "\n",
    "        self.disp_score = torch.range(0, max_disp - 1)  # [D]\n",
    "        self.disp_score = self.disp_score.unsqueeze(0).unsqueeze(2).unsqueeze(3)  # [1, D, 1, 1]\n",
    "\n",
    "    def forward(self, prob):\n",
    "        disp_score = self.disp_score.expand_as(prob).type_as(prob)  # [B, D, H, W]\n",
    "        out = torch.sum(disp_score * prob, dim=1)  # [B, H, W]\n",
    "        return out\n",
    "\n",
    "\n",
    "class Hourglass(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net1 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1, dilation=1, use_relu=True),\n",
    "            Conv3dBn(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=False)\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1, dilation=1, use_relu=True),\n",
    "            Conv3dBn(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True)\n",
    "        )\n",
    "        self.net3 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "            nn.BatchNorm3d(num_features=64)\n",
    "            # nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.net4 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "            nn.BatchNorm3d(num_features=32)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, scale1=None, scale2=None, scale3=None):\n",
    "        net1_out = self.net1(inputs)  # [B, 64, 1/8D, 1/8H, 1/8W]\n",
    "\n",
    "        if scale1 is not None:\n",
    "            net1_out = F.relu(net1_out + scale1, inplace=True)\n",
    "        else:\n",
    "            net1_out = F.relu(net1_out, inplace=True)\n",
    "\n",
    "        net2_out = self.net2(net1_out)  # [B, 64, 1/16D, 1/16H, 1/16W]\n",
    "        net3_out = self.net3(net2_out)  # [B, 64, 1/8D, 1/8H, 1/8W]\n",
    "\n",
    "        if scale2 is not None:\n",
    "            net3_out = F.relu(net3_out + scale2, inplace=True)\n",
    "        else:\n",
    "            net3_out = F.relu(net3_out + net1_out, inplace=True)\n",
    "\n",
    "        net4_out = self.net4(net3_out)\n",
    "\n",
    "        if scale3 is not None:\n",
    "            net4_out = net4_out + scale3\n",
    "\n",
    "        return net1_out, net3_out, net4_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GCNetPlus(nn.Module):\n",
    "\n",
    "    def __init__(self, max_disp, k=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cost_net = CostNet(k=k)\n",
    "        self.stackedhourglass = StackedHourglass(max_disp)\n",
    "        self.D = max_disp\n",
    "\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        original_size = [self.D, left_img.size(2), left_img.size(3)]\n",
    "\n",
    "        left_cost = self.cost_net(left_img)  # [B, 32, 1/4H, 1/4W]\n",
    "        right_cost = self.cost_net(right_img)  # [B, 32, 1/4H, 1/4W]\n",
    "        #cost = torch.cat([left_cost, right_cost], dim=1)  # [B, 64, 1/4H, 1/4W]\n",
    "        # B, C, H, W = cost.size()\n",
    "\n",
    "        print(\"lcost, rcost\", left_cost.shape, right_cost.shape) # torch.Size([4, 32, 96, 320]) torch.Size([4, 32, 96, 320])\n",
    "        # print(left_cost[0, 0, :3, :3])\n",
    "\n",
    "        B, C, H, W = left_cost.size()\n",
    "        print(\"B, C, H, W\", B, C, H, W)\n",
    "        cost_volume = torch.zeros(B, C * 2, self.D // 4, H, W).type_as(left_cost)  # [B, 64, D, 1/4H, 1/4W]\n",
    "        print(\"cost_volume1\", cost_volume.shape)\n",
    "        # for i in range(self.D // 4):\n",
    "        #     cost_volume[:, :, i, :, i:] = cost[:, :, :, i:]\n",
    "        #\"\"\"\n",
    "        for i in range(self.D // 4):\n",
    "            if i > 0:\n",
    "                cost_volume[:, :C, i, :, i:] = left_cost[:, :, :, i:]\n",
    "                cost_volume[:, C:, i, :, i:] = right_cost[:, :, :, :-i]\n",
    "            else:\n",
    "                cost_volume[:, :C, i, :, :] = left_cost\n",
    "                cost_volume[:, C:, i, :, :] = right_cost\n",
    "       # \"\"\"\n",
    "        print(\"cost_volume2\", cost_volume.shape)\n",
    "        disp1, disp2, disp3 = self.stackedhourglass(cost_volume, out_size=original_size)\n",
    "        print(\"disp\", disp1.shape, disp2.shape, disp3.shape)\n",
    "        return cost_volume #disp1, disp2, disp3\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6ce2b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 384, 1280]) torch.Size([4, 3, 384, 1280])\n"
     ]
    }
   ],
   "source": [
    "H= 192*2\n",
    "W=640*2\n",
    "rinp = torch.randn(4, 3, 192*2, 640*2)\n",
    "linp = torch.randn(4, 3, 192*2, 640*2)\n",
    "print(rinp.shape, linp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57900849",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2032453164.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [43]\u001b[0;36m\u001b[0m\n\u001b[0;31m    inp, c0 c1 torch.Size([2, 3, 384, 1280]) torch.Size([2, 32, 192, 640]) torch.Size([2, 32, 192, 640])\u001b[0m\n\u001b[0m                                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "inp, c0 c1 torch.Size([2, 3, 384, 1280]) torch.Size([2, 32, 192, 640]) torch.Size([2, 32, 192, 640])\n",
    "c2 c3 c4 torch.Size([2, 64, 96, 320]) torch.Size([2, 128, 96, 320]) torch.Size([2, 128, 96, 320])\n",
    "costnet2 torch.Size([2, 64, 96, 320]) torch.Size([2, 128, 96, 320]) torch.Size([2, 128, 96, 320])\n",
    "costnet3 torch.Size([2, 320, 96, 320])\n",
    "fusion out torch.Size([2, 32, 96, 320])\n",
    "inp, c0 c1 torch.Size([2, 3, 384, 1280]) torch.Size([2, 32, 192, 640]) torch.Size([2, 32, 192, 640])\n",
    "c2 c3 c4 torch.Size([2, 64, 96, 320]) torch.Size([2, 128, 96, 320]) torch.Size([2, 128, 96, 320])\n",
    "costnet2 torch.Size([2, 64, 96, 320]) torch.Size([2, 128, 96, 320]) torch.Size([2, 128, 96, 320])\n",
    "costnet3 torch.Size([2, 320, 96, 320])\n",
    "fusion out torch.Size([2, 32, 96, 320])\n",
    "lcost, rcost torch.Size([2, 32, 96, 320]) torch.Size([2, 32, 96, 320])\n",
    "B, C, H, W 2 32 96 320\n",
    "cost_volume1 torch.Size([2, 64, 48, 96, 320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "417b88cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fs/xcd148y15494hf9gnyplx3y00000gn/T/ipykernel_2741/3801135926.py:314: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  self.disp_score = torch.range(0, max_disp - 1)  # [D]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp, c0 c1 torch.Size([2, 3, 384, 1280]) torch.Size([2, 32, 192, 640]) torch.Size([2, 32, 192, 640])\n",
      "c2 c3 c4 torch.Size([2, 64, 96, 320]) torch.Size([2, 128, 96, 320]) torch.Size([2, 128, 96, 320])\n",
      "costnet2 torch.Size([2, 64, 96, 320]) torch.Size([2, 128, 96, 320]) torch.Size([2, 128, 96, 320])\n",
      "costnet3 torch.Size([2, 320, 96, 320])\n",
      "fusion out torch.Size([2, 32, 96, 320])\n",
      "inp, c0 c1 torch.Size([2, 3, 384, 1280]) torch.Size([2, 32, 192, 640]) torch.Size([2, 32, 192, 640])\n",
      "c2 c3 c4 torch.Size([2, 64, 96, 320]) torch.Size([2, 128, 96, 320]) torch.Size([2, 128, 96, 320])\n",
      "costnet2 torch.Size([2, 64, 96, 320]) torch.Size([2, 128, 96, 320]) torch.Size([2, 128, 96, 320])\n",
      "costnet3 torch.Size([2, 320, 96, 320])\n",
      "fusion out torch.Size([2, 32, 96, 320])\n",
      "lcost, rcost torch.Size([2, 32, 96, 320]) torch.Size([2, 32, 96, 320])\n",
      "B, C, H, W 2 32 96 320\n",
      "cost_volume1 torch.Size([2, 64, 48, 96, 320])\n",
      "cost_volume2 torch.Size([2, 64, 48, 96, 320])\n",
      "disp torch.Size([2, 384, 1280]) torch.Size([2, 384, 1280]) torch.Size([2, 384, 1280])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 192, 640]             864\n",
      "       BatchNorm2d-2         [-1, 32, 192, 640]              64\n",
      "              SiLU-3         [-1, 32, 192, 640]               0\n",
      "          Conv2dBn-4         [-1, 32, 192, 640]               0\n",
      "            Conv2d-5         [-1, 32, 192, 640]           9,216\n",
      "       BatchNorm2d-6         [-1, 32, 192, 640]              64\n",
      "              SiLU-7         [-1, 32, 192, 640]               0\n",
      "          Conv2dBn-8         [-1, 32, 192, 640]               0\n",
      "            Conv2d-9         [-1, 32, 192, 640]           9,216\n",
      "      BatchNorm2d-10         [-1, 32, 192, 640]              64\n",
      "             SiLU-11         [-1, 32, 192, 640]               0\n",
      "         Conv2dBn-12         [-1, 32, 192, 640]               0\n",
      "           Conv2d-13         [-1, 32, 192, 640]           1,024\n",
      "      BatchNorm2d-14         [-1, 32, 192, 640]              64\n",
      "             SiLU-15         [-1, 32, 192, 640]               0\n",
      "           Conv2d-16         [-1, 32, 192, 640]             288\n",
      "           Conv2d-17         [-1, 32, 192, 640]           1,024\n",
      "depthwise_separable_conv-18         [-1, 32, 192, 640]               0\n",
      "      BatchNorm2d-19         [-1, 32, 192, 640]              64\n",
      "             SiLU-20         [-1, 32, 192, 640]               0\n",
      "           Conv2d-21          [-1, 8, 192, 640]             264\n",
      "             SiLU-22          [-1, 8, 192, 640]               0\n",
      "           Conv2d-23         [-1, 32, 192, 640]             288\n",
      "          Sigmoid-24         [-1, 32, 192, 640]               0\n",
      "Squeeze_excitation_layer-25         [-1, 32, 192, 640]               0\n",
      "           Conv2d-26         [-1, 32, 192, 640]           1,024\n",
      "      BatchNorm2d-27         [-1, 32, 192, 640]              64\n",
      "          Dropout-28         [-1, 32, 192, 640]               0\n",
      "   MBConv2d_block-29         [-1, 32, 192, 640]               0\n",
      "           Conv2d-30         [-1, 32, 192, 640]           9,216\n",
      "      BatchNorm2d-31         [-1, 32, 192, 640]              64\n",
      "             SiLU-32         [-1, 32, 192, 640]               0\n",
      "         Conv2dBn-33         [-1, 32, 192, 640]               0\n",
      "           Conv2d-34         [-1, 32, 192, 640]           9,216\n",
      "      BatchNorm2d-35         [-1, 32, 192, 640]              64\n",
      "         Conv2dBn-36         [-1, 32, 192, 640]               0\n",
      "    ResidualBlock-37         [-1, 32, 192, 640]               0\n",
      "           Conv2d-38         [-1, 32, 192, 640]           9,216\n",
      "      BatchNorm2d-39         [-1, 32, 192, 640]              64\n",
      "             SiLU-40         [-1, 32, 192, 640]               0\n",
      "         Conv2dBn-41         [-1, 32, 192, 640]               0\n",
      "           Conv2d-42         [-1, 32, 192, 640]           9,216\n",
      "      BatchNorm2d-43         [-1, 32, 192, 640]              64\n",
      "         Conv2dBn-44         [-1, 32, 192, 640]               0\n",
      "    ResidualBlock-45         [-1, 32, 192, 640]               0\n",
      "           Conv2d-46         [-1, 32, 192, 640]           9,216\n",
      "      BatchNorm2d-47         [-1, 32, 192, 640]              64\n",
      "             SiLU-48         [-1, 32, 192, 640]               0\n",
      "         Conv2dBn-49         [-1, 32, 192, 640]               0\n",
      "           Conv2d-50         [-1, 32, 192, 640]           9,216\n",
      "      BatchNorm2d-51         [-1, 32, 192, 640]              64\n",
      "         Conv2dBn-52         [-1, 32, 192, 640]               0\n",
      "    ResidualBlock-53         [-1, 32, 192, 640]               0\n",
      "    StackedBlocks-54         [-1, 32, 192, 640]               0\n",
      "           Conv2d-55         [-1, 32, 192, 640]           1,024\n",
      "      BatchNorm2d-56         [-1, 32, 192, 640]              64\n",
      "             SiLU-57         [-1, 32, 192, 640]               0\n",
      "           Conv2d-58         [-1, 32, 192, 640]             288\n",
      "           Conv2d-59         [-1, 32, 192, 640]           1,024\n",
      "depthwise_separable_conv-60         [-1, 32, 192, 640]               0\n",
      "      BatchNorm2d-61         [-1, 32, 192, 640]              64\n",
      "             SiLU-62         [-1, 32, 192, 640]               0\n",
      "           Conv2d-63          [-1, 8, 192, 640]             264\n",
      "             SiLU-64          [-1, 8, 192, 640]               0\n",
      "           Conv2d-65         [-1, 32, 192, 640]             288\n",
      "          Sigmoid-66         [-1, 32, 192, 640]               0\n",
      "Squeeze_excitation_layer-67         [-1, 32, 192, 640]               0\n",
      "           Conv2d-68         [-1, 32, 192, 640]           1,024\n",
      "      BatchNorm2d-69         [-1, 32, 192, 640]              64\n",
      "          Dropout-70         [-1, 32, 192, 640]               0\n",
      "   MBConv2d_block-71         [-1, 32, 192, 640]               0\n",
      "           Conv2d-72          [-1, 64, 96, 320]          18,432\n",
      "      BatchNorm2d-73          [-1, 64, 96, 320]             128\n",
      "             SiLU-74          [-1, 64, 96, 320]               0\n",
      "         Conv2dBn-75          [-1, 64, 96, 320]               0\n",
      "           Conv2d-76          [-1, 64, 96, 320]          36,864\n",
      "      BatchNorm2d-77          [-1, 64, 96, 320]             128\n",
      "         Conv2dBn-78          [-1, 64, 96, 320]               0\n",
      "           Conv2d-79          [-1, 64, 96, 320]           2,048\n",
      "      BatchNorm2d-80          [-1, 64, 96, 320]             128\n",
      "         Conv2dBn-81          [-1, 64, 96, 320]               0\n",
      "    ResidualBlock-82          [-1, 64, 96, 320]               0\n",
      "           Conv2d-83          [-1, 64, 96, 320]          36,864\n",
      "      BatchNorm2d-84          [-1, 64, 96, 320]             128\n",
      "             SiLU-85          [-1, 64, 96, 320]               0\n",
      "         Conv2dBn-86          [-1, 64, 96, 320]               0\n",
      "           Conv2d-87          [-1, 64, 96, 320]          36,864\n",
      "      BatchNorm2d-88          [-1, 64, 96, 320]             128\n",
      "         Conv2dBn-89          [-1, 64, 96, 320]               0\n",
      "    ResidualBlock-90          [-1, 64, 96, 320]               0\n",
      "           Conv2d-91          [-1, 64, 96, 320]          36,864\n",
      "      BatchNorm2d-92          [-1, 64, 96, 320]             128\n",
      "             SiLU-93          [-1, 64, 96, 320]               0\n",
      "         Conv2dBn-94          [-1, 64, 96, 320]               0\n",
      "           Conv2d-95          [-1, 64, 96, 320]          36,864\n",
      "      BatchNorm2d-96          [-1, 64, 96, 320]             128\n",
      "         Conv2dBn-97          [-1, 64, 96, 320]               0\n",
      "    ResidualBlock-98          [-1, 64, 96, 320]               0\n",
      "    StackedBlocks-99          [-1, 64, 96, 320]               0\n",
      "          Conv2d-100          [-1, 64, 96, 320]           4,096\n",
      "     BatchNorm2d-101          [-1, 64, 96, 320]             128\n",
      "            SiLU-102          [-1, 64, 96, 320]               0\n",
      "          Conv2d-103          [-1, 64, 96, 320]             576\n",
      "          Conv2d-104          [-1, 64, 96, 320]           4,096\n",
      "depthwise_separable_conv-105          [-1, 64, 96, 320]               0\n",
      "     BatchNorm2d-106          [-1, 64, 96, 320]             128\n",
      "            SiLU-107          [-1, 64, 96, 320]               0\n",
      "          Conv2d-108          [-1, 16, 96, 320]           1,040\n",
      "            SiLU-109          [-1, 16, 96, 320]               0\n",
      "          Conv2d-110          [-1, 64, 96, 320]           1,088\n",
      "         Sigmoid-111          [-1, 64, 96, 320]               0\n",
      "Squeeze_excitation_layer-112          [-1, 64, 96, 320]               0\n",
      "          Conv2d-113          [-1, 64, 96, 320]           4,096\n",
      "     BatchNorm2d-114          [-1, 64, 96, 320]             128\n",
      "         Dropout-115          [-1, 64, 96, 320]               0\n",
      "  MBConv2d_block-116          [-1, 64, 96, 320]               0\n",
      "          Conv2d-117         [-1, 128, 96, 320]          73,728\n",
      "     BatchNorm2d-118         [-1, 128, 96, 320]             256\n",
      "            SiLU-119         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-120         [-1, 128, 96, 320]               0\n",
      "          Conv2d-121         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-122         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-123         [-1, 128, 96, 320]               0\n",
      "          Conv2d-124         [-1, 128, 96, 320]           8,192\n",
      "     BatchNorm2d-125         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-126         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-127         [-1, 128, 96, 320]               0\n",
      "          Conv2d-128         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-129         [-1, 128, 96, 320]             256\n",
      "            SiLU-130         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-131         [-1, 128, 96, 320]               0\n",
      "          Conv2d-132         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-133         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-134         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-135         [-1, 128, 96, 320]               0\n",
      "          Conv2d-136         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-137         [-1, 128, 96, 320]             256\n",
      "            SiLU-138         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-139         [-1, 128, 96, 320]               0\n",
      "          Conv2d-140         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-141         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-142         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-143         [-1, 128, 96, 320]               0\n",
      "   StackedBlocks-144         [-1, 128, 96, 320]               0\n",
      "          Conv2d-145         [-1, 128, 96, 320]          16,384\n",
      "     BatchNorm2d-146         [-1, 128, 96, 320]             256\n",
      "            SiLU-147         [-1, 128, 96, 320]               0\n",
      "          Conv2d-148         [-1, 128, 96, 320]           1,152\n",
      "          Conv2d-149         [-1, 128, 96, 320]          16,384\n",
      "depthwise_separable_conv-150         [-1, 128, 96, 320]               0\n",
      "     BatchNorm2d-151         [-1, 128, 96, 320]             256\n",
      "            SiLU-152         [-1, 128, 96, 320]               0\n",
      "          Conv2d-153          [-1, 32, 96, 320]           4,128\n",
      "            SiLU-154          [-1, 32, 96, 320]               0\n",
      "          Conv2d-155         [-1, 128, 96, 320]           4,224\n",
      "         Sigmoid-156         [-1, 128, 96, 320]               0\n",
      "Squeeze_excitation_layer-157         [-1, 128, 96, 320]               0\n",
      "          Conv2d-158         [-1, 128, 96, 320]          16,384\n",
      "     BatchNorm2d-159         [-1, 128, 96, 320]             256\n",
      "         Dropout-160         [-1, 128, 96, 320]               0\n",
      "  MBConv2d_block-161         [-1, 128, 96, 320]               0\n",
      "          Conv2d-162         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-163         [-1, 128, 96, 320]             256\n",
      "            SiLU-164         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-165         [-1, 128, 96, 320]               0\n",
      "          Conv2d-166         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-167         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-168         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-169         [-1, 128, 96, 320]               0\n",
      "          Conv2d-170         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-171         [-1, 128, 96, 320]             256\n",
      "            SiLU-172         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-173         [-1, 128, 96, 320]               0\n",
      "          Conv2d-174         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-175         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-176         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-177         [-1, 128, 96, 320]               0\n",
      "          Conv2d-178         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-179         [-1, 128, 96, 320]             256\n",
      "            SiLU-180         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-181         [-1, 128, 96, 320]               0\n",
      "          Conv2d-182         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-183         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-184         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-185         [-1, 128, 96, 320]               0\n",
      "   StackedBlocks-186         [-1, 128, 96, 320]               0\n",
      "             CNN-187  [[-1, 64, 96, 320], [-1, 128, 96, 320]]               0\n",
      "       AvgPool2d-188            [-1, 128, 1, 5]               0\n",
      "          Conv2d-189             [-1, 32, 1, 5]          36,864\n",
      "     BatchNorm2d-190             [-1, 32, 1, 5]              64\n",
      "            SiLU-191             [-1, 32, 1, 5]               0\n",
      "        Conv2dBn-192             [-1, 32, 1, 5]               0\n",
      "       AvgPool2d-193           [-1, 128, 3, 10]               0\n",
      "          Conv2d-194            [-1, 32, 3, 10]          36,864\n",
      "     BatchNorm2d-195            [-1, 32, 3, 10]              64\n",
      "            SiLU-196            [-1, 32, 3, 10]               0\n",
      "        Conv2dBn-197            [-1, 32, 3, 10]               0\n",
      "       AvgPool2d-198           [-1, 128, 6, 20]               0\n",
      "          Conv2d-199            [-1, 32, 6, 20]          36,864\n",
      "     BatchNorm2d-200            [-1, 32, 6, 20]              64\n",
      "            SiLU-201            [-1, 32, 6, 20]               0\n",
      "        Conv2dBn-202            [-1, 32, 6, 20]               0\n",
      "       AvgPool2d-203          [-1, 128, 12, 40]               0\n",
      "          Conv2d-204           [-1, 32, 12, 40]          36,864\n",
      "     BatchNorm2d-205           [-1, 32, 12, 40]              64\n",
      "            SiLU-206           [-1, 32, 12, 40]               0\n",
      "        Conv2dBn-207           [-1, 32, 12, 40]               0\n",
      "             SPP-208         [-1, 128, 96, 320]               0\n",
      "          Conv2d-209         [-1, 128, 96, 320]         368,640\n",
      "     BatchNorm2d-210         [-1, 128, 96, 320]             256\n",
      "            SiLU-211         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-212         [-1, 128, 96, 320]               0\n",
      "          Conv2d-213          [-1, 32, 96, 320]           4,096\n",
      "         CostNet-214          [-1, 32, 96, 320]               0\n",
      "          Conv2d-215         [-1, 32, 192, 640]             864\n",
      "     BatchNorm2d-216         [-1, 32, 192, 640]              64\n",
      "            SiLU-217         [-1, 32, 192, 640]               0\n",
      "        Conv2dBn-218         [-1, 32, 192, 640]               0\n",
      "          Conv2d-219         [-1, 32, 192, 640]           9,216\n",
      "     BatchNorm2d-220         [-1, 32, 192, 640]              64\n",
      "            SiLU-221         [-1, 32, 192, 640]               0\n",
      "        Conv2dBn-222         [-1, 32, 192, 640]               0\n",
      "          Conv2d-223         [-1, 32, 192, 640]           9,216\n",
      "     BatchNorm2d-224         [-1, 32, 192, 640]              64\n",
      "            SiLU-225         [-1, 32, 192, 640]               0\n",
      "        Conv2dBn-226         [-1, 32, 192, 640]               0\n",
      "          Conv2d-227         [-1, 32, 192, 640]           1,024\n",
      "     BatchNorm2d-228         [-1, 32, 192, 640]              64\n",
      "            SiLU-229         [-1, 32, 192, 640]               0\n",
      "          Conv2d-230         [-1, 32, 192, 640]             288\n",
      "          Conv2d-231         [-1, 32, 192, 640]           1,024\n",
      "depthwise_separable_conv-232         [-1, 32, 192, 640]               0\n",
      "     BatchNorm2d-233         [-1, 32, 192, 640]              64\n",
      "            SiLU-234         [-1, 32, 192, 640]               0\n",
      "          Conv2d-235          [-1, 8, 192, 640]             264\n",
      "            SiLU-236          [-1, 8, 192, 640]               0\n",
      "          Conv2d-237         [-1, 32, 192, 640]             288\n",
      "         Sigmoid-238         [-1, 32, 192, 640]               0\n",
      "Squeeze_excitation_layer-239         [-1, 32, 192, 640]               0\n",
      "          Conv2d-240         [-1, 32, 192, 640]           1,024\n",
      "     BatchNorm2d-241         [-1, 32, 192, 640]              64\n",
      "         Dropout-242         [-1, 32, 192, 640]               0\n",
      "  MBConv2d_block-243         [-1, 32, 192, 640]               0\n",
      "          Conv2d-244         [-1, 32, 192, 640]           9,216\n",
      "     BatchNorm2d-245         [-1, 32, 192, 640]              64\n",
      "            SiLU-246         [-1, 32, 192, 640]               0\n",
      "        Conv2dBn-247         [-1, 32, 192, 640]               0\n",
      "          Conv2d-248         [-1, 32, 192, 640]           9,216\n",
      "     BatchNorm2d-249         [-1, 32, 192, 640]              64\n",
      "        Conv2dBn-250         [-1, 32, 192, 640]               0\n",
      "   ResidualBlock-251         [-1, 32, 192, 640]               0\n",
      "          Conv2d-252         [-1, 32, 192, 640]           9,216\n",
      "     BatchNorm2d-253         [-1, 32, 192, 640]              64\n",
      "            SiLU-254         [-1, 32, 192, 640]               0\n",
      "        Conv2dBn-255         [-1, 32, 192, 640]               0\n",
      "          Conv2d-256         [-1, 32, 192, 640]           9,216\n",
      "     BatchNorm2d-257         [-1, 32, 192, 640]              64\n",
      "        Conv2dBn-258         [-1, 32, 192, 640]               0\n",
      "   ResidualBlock-259         [-1, 32, 192, 640]               0\n",
      "          Conv2d-260         [-1, 32, 192, 640]           9,216\n",
      "     BatchNorm2d-261         [-1, 32, 192, 640]              64\n",
      "            SiLU-262         [-1, 32, 192, 640]               0\n",
      "        Conv2dBn-263         [-1, 32, 192, 640]               0\n",
      "          Conv2d-264         [-1, 32, 192, 640]           9,216\n",
      "     BatchNorm2d-265         [-1, 32, 192, 640]              64\n",
      "        Conv2dBn-266         [-1, 32, 192, 640]               0\n",
      "   ResidualBlock-267         [-1, 32, 192, 640]               0\n",
      "   StackedBlocks-268         [-1, 32, 192, 640]               0\n",
      "          Conv2d-269         [-1, 32, 192, 640]           1,024\n",
      "     BatchNorm2d-270         [-1, 32, 192, 640]              64\n",
      "            SiLU-271         [-1, 32, 192, 640]               0\n",
      "          Conv2d-272         [-1, 32, 192, 640]             288\n",
      "          Conv2d-273         [-1, 32, 192, 640]           1,024\n",
      "depthwise_separable_conv-274         [-1, 32, 192, 640]               0\n",
      "     BatchNorm2d-275         [-1, 32, 192, 640]              64\n",
      "            SiLU-276         [-1, 32, 192, 640]               0\n",
      "          Conv2d-277          [-1, 8, 192, 640]             264\n",
      "            SiLU-278          [-1, 8, 192, 640]               0\n",
      "          Conv2d-279         [-1, 32, 192, 640]             288\n",
      "         Sigmoid-280         [-1, 32, 192, 640]               0\n",
      "Squeeze_excitation_layer-281         [-1, 32, 192, 640]               0\n",
      "          Conv2d-282         [-1, 32, 192, 640]           1,024\n",
      "     BatchNorm2d-283         [-1, 32, 192, 640]              64\n",
      "         Dropout-284         [-1, 32, 192, 640]               0\n",
      "  MBConv2d_block-285         [-1, 32, 192, 640]               0\n",
      "          Conv2d-286          [-1, 64, 96, 320]          18,432\n",
      "     BatchNorm2d-287          [-1, 64, 96, 320]             128\n",
      "            SiLU-288          [-1, 64, 96, 320]               0\n",
      "        Conv2dBn-289          [-1, 64, 96, 320]               0\n",
      "          Conv2d-290          [-1, 64, 96, 320]          36,864\n",
      "     BatchNorm2d-291          [-1, 64, 96, 320]             128\n",
      "        Conv2dBn-292          [-1, 64, 96, 320]               0\n",
      "          Conv2d-293          [-1, 64, 96, 320]           2,048\n",
      "     BatchNorm2d-294          [-1, 64, 96, 320]             128\n",
      "        Conv2dBn-295          [-1, 64, 96, 320]               0\n",
      "   ResidualBlock-296          [-1, 64, 96, 320]               0\n",
      "          Conv2d-297          [-1, 64, 96, 320]          36,864\n",
      "     BatchNorm2d-298          [-1, 64, 96, 320]             128\n",
      "            SiLU-299          [-1, 64, 96, 320]               0\n",
      "        Conv2dBn-300          [-1, 64, 96, 320]               0\n",
      "          Conv2d-301          [-1, 64, 96, 320]          36,864\n",
      "     BatchNorm2d-302          [-1, 64, 96, 320]             128\n",
      "        Conv2dBn-303          [-1, 64, 96, 320]               0\n",
      "   ResidualBlock-304          [-1, 64, 96, 320]               0\n",
      "          Conv2d-305          [-1, 64, 96, 320]          36,864\n",
      "     BatchNorm2d-306          [-1, 64, 96, 320]             128\n",
      "            SiLU-307          [-1, 64, 96, 320]               0\n",
      "        Conv2dBn-308          [-1, 64, 96, 320]               0\n",
      "          Conv2d-309          [-1, 64, 96, 320]          36,864\n",
      "     BatchNorm2d-310          [-1, 64, 96, 320]             128\n",
      "        Conv2dBn-311          [-1, 64, 96, 320]               0\n",
      "   ResidualBlock-312          [-1, 64, 96, 320]               0\n",
      "   StackedBlocks-313          [-1, 64, 96, 320]               0\n",
      "          Conv2d-314          [-1, 64, 96, 320]           4,096\n",
      "     BatchNorm2d-315          [-1, 64, 96, 320]             128\n",
      "            SiLU-316          [-1, 64, 96, 320]               0\n",
      "          Conv2d-317          [-1, 64, 96, 320]             576\n",
      "          Conv2d-318          [-1, 64, 96, 320]           4,096\n",
      "depthwise_separable_conv-319          [-1, 64, 96, 320]               0\n",
      "     BatchNorm2d-320          [-1, 64, 96, 320]             128\n",
      "            SiLU-321          [-1, 64, 96, 320]               0\n",
      "          Conv2d-322          [-1, 16, 96, 320]           1,040\n",
      "            SiLU-323          [-1, 16, 96, 320]               0\n",
      "          Conv2d-324          [-1, 64, 96, 320]           1,088\n",
      "         Sigmoid-325          [-1, 64, 96, 320]               0\n",
      "Squeeze_excitation_layer-326          [-1, 64, 96, 320]               0\n",
      "          Conv2d-327          [-1, 64, 96, 320]           4,096\n",
      "     BatchNorm2d-328          [-1, 64, 96, 320]             128\n",
      "         Dropout-329          [-1, 64, 96, 320]               0\n",
      "  MBConv2d_block-330          [-1, 64, 96, 320]               0\n",
      "          Conv2d-331         [-1, 128, 96, 320]          73,728\n",
      "     BatchNorm2d-332         [-1, 128, 96, 320]             256\n",
      "            SiLU-333         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-334         [-1, 128, 96, 320]               0\n",
      "          Conv2d-335         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-336         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-337         [-1, 128, 96, 320]               0\n",
      "          Conv2d-338         [-1, 128, 96, 320]           8,192\n",
      "     BatchNorm2d-339         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-340         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-341         [-1, 128, 96, 320]               0\n",
      "          Conv2d-342         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-343         [-1, 128, 96, 320]             256\n",
      "            SiLU-344         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-345         [-1, 128, 96, 320]               0\n",
      "          Conv2d-346         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-347         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-348         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-349         [-1, 128, 96, 320]               0\n",
      "          Conv2d-350         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-351         [-1, 128, 96, 320]             256\n",
      "            SiLU-352         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-353         [-1, 128, 96, 320]               0\n",
      "          Conv2d-354         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-355         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-356         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-357         [-1, 128, 96, 320]               0\n",
      "   StackedBlocks-358         [-1, 128, 96, 320]               0\n",
      "          Conv2d-359         [-1, 128, 96, 320]          16,384\n",
      "     BatchNorm2d-360         [-1, 128, 96, 320]             256\n",
      "            SiLU-361         [-1, 128, 96, 320]               0\n",
      "          Conv2d-362         [-1, 128, 96, 320]           1,152\n",
      "          Conv2d-363         [-1, 128, 96, 320]          16,384\n",
      "depthwise_separable_conv-364         [-1, 128, 96, 320]               0\n",
      "     BatchNorm2d-365         [-1, 128, 96, 320]             256\n",
      "            SiLU-366         [-1, 128, 96, 320]               0\n",
      "          Conv2d-367          [-1, 32, 96, 320]           4,128\n",
      "            SiLU-368          [-1, 32, 96, 320]               0\n",
      "          Conv2d-369         [-1, 128, 96, 320]           4,224\n",
      "         Sigmoid-370         [-1, 128, 96, 320]               0\n",
      "Squeeze_excitation_layer-371         [-1, 128, 96, 320]               0\n",
      "          Conv2d-372         [-1, 128, 96, 320]          16,384\n",
      "     BatchNorm2d-373         [-1, 128, 96, 320]             256\n",
      "         Dropout-374         [-1, 128, 96, 320]               0\n",
      "  MBConv2d_block-375         [-1, 128, 96, 320]               0\n",
      "          Conv2d-376         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-377         [-1, 128, 96, 320]             256\n",
      "            SiLU-378         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-379         [-1, 128, 96, 320]               0\n",
      "          Conv2d-380         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-381         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-382         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-383         [-1, 128, 96, 320]               0\n",
      "          Conv2d-384         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-385         [-1, 128, 96, 320]             256\n",
      "            SiLU-386         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-387         [-1, 128, 96, 320]               0\n",
      "          Conv2d-388         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-389         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-390         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-391         [-1, 128, 96, 320]               0\n",
      "          Conv2d-392         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-393         [-1, 128, 96, 320]             256\n",
      "            SiLU-394         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-395         [-1, 128, 96, 320]               0\n",
      "          Conv2d-396         [-1, 128, 96, 320]         147,456\n",
      "     BatchNorm2d-397         [-1, 128, 96, 320]             256\n",
      "        Conv2dBn-398         [-1, 128, 96, 320]               0\n",
      "   ResidualBlock-399         [-1, 128, 96, 320]               0\n",
      "   StackedBlocks-400         [-1, 128, 96, 320]               0\n",
      "             CNN-401  [[-1, 64, 96, 320], [-1, 128, 96, 320]]               0\n",
      "       AvgPool2d-402            [-1, 128, 1, 5]               0\n",
      "          Conv2d-403             [-1, 32, 1, 5]          36,864\n",
      "     BatchNorm2d-404             [-1, 32, 1, 5]              64\n",
      "            SiLU-405             [-1, 32, 1, 5]               0\n",
      "        Conv2dBn-406             [-1, 32, 1, 5]               0\n",
      "       AvgPool2d-407           [-1, 128, 3, 10]               0\n",
      "          Conv2d-408            [-1, 32, 3, 10]          36,864\n",
      "     BatchNorm2d-409            [-1, 32, 3, 10]              64\n",
      "            SiLU-410            [-1, 32, 3, 10]               0\n",
      "        Conv2dBn-411            [-1, 32, 3, 10]               0\n",
      "       AvgPool2d-412           [-1, 128, 6, 20]               0\n",
      "          Conv2d-413            [-1, 32, 6, 20]          36,864\n",
      "     BatchNorm2d-414            [-1, 32, 6, 20]              64\n",
      "            SiLU-415            [-1, 32, 6, 20]               0\n",
      "        Conv2dBn-416            [-1, 32, 6, 20]               0\n",
      "       AvgPool2d-417          [-1, 128, 12, 40]               0\n",
      "          Conv2d-418           [-1, 32, 12, 40]          36,864\n",
      "     BatchNorm2d-419           [-1, 32, 12, 40]              64\n",
      "            SiLU-420           [-1, 32, 12, 40]               0\n",
      "        Conv2dBn-421           [-1, 32, 12, 40]               0\n",
      "             SPP-422         [-1, 128, 96, 320]               0\n",
      "          Conv2d-423         [-1, 128, 96, 320]         368,640\n",
      "     BatchNorm2d-424         [-1, 128, 96, 320]             256\n",
      "            SiLU-425         [-1, 128, 96, 320]               0\n",
      "        Conv2dBn-426         [-1, 128, 96, 320]               0\n",
      "          Conv2d-427          [-1, 32, 96, 320]           4,096\n",
      "         CostNet-428          [-1, 32, 96, 320]               0\n",
      "          Conv3d-429      [-1, 32, 48, 96, 320]          55,296\n",
      "     BatchNorm3d-430      [-1, 32, 48, 96, 320]              64\n",
      "            SiLU-431      [-1, 32, 48, 96, 320]               0\n",
      "        Conv3dBn-432      [-1, 32, 48, 96, 320]               0\n",
      "          Conv3d-433      [-1, 32, 48, 96, 320]          27,648\n",
      "     BatchNorm3d-434      [-1, 32, 48, 96, 320]              64\n",
      "            SiLU-435      [-1, 32, 48, 96, 320]               0\n",
      "        Conv3dBn-436      [-1, 32, 48, 96, 320]               0\n",
      "          Conv3d-437      [-1, 32, 48, 96, 320]          27,648\n",
      "     BatchNorm3d-438      [-1, 32, 48, 96, 320]              64\n",
      "            SiLU-439      [-1, 32, 48, 96, 320]               0\n",
      "        Conv3dBn-440      [-1, 32, 48, 96, 320]               0\n",
      "          Conv3d-441      [-1, 32, 48, 96, 320]          27,648\n",
      "     BatchNorm3d-442      [-1, 32, 48, 96, 320]              64\n",
      "        Conv3dBn-443      [-1, 32, 48, 96, 320]               0\n",
      "          Conv3d-444      [-1, 64, 24, 48, 160]          55,296\n",
      "     BatchNorm3d-445      [-1, 64, 24, 48, 160]             128\n",
      "            SiLU-446      [-1, 64, 24, 48, 160]               0\n",
      "        Conv3dBn-447      [-1, 64, 24, 48, 160]               0\n",
      "          Conv3d-448      [-1, 64, 24, 48, 160]         110,592\n",
      "     BatchNorm3d-449      [-1, 64, 24, 48, 160]             128\n",
      "        Conv3dBn-450      [-1, 64, 24, 48, 160]               0\n",
      "          Conv3d-451       [-1, 64, 12, 24, 80]         110,592\n",
      "     BatchNorm3d-452       [-1, 64, 12, 24, 80]             128\n",
      "            SiLU-453       [-1, 64, 12, 24, 80]               0\n",
      "        Conv3dBn-454       [-1, 64, 12, 24, 80]               0\n",
      "          Conv3d-455       [-1, 64, 12, 24, 80]         110,592\n",
      "     BatchNorm3d-456       [-1, 64, 12, 24, 80]             128\n",
      "            SiLU-457       [-1, 64, 12, 24, 80]               0\n",
      "        Conv3dBn-458       [-1, 64, 12, 24, 80]               0\n",
      " ConvTranspose3d-459      [-1, 64, 24, 48, 160]         110,592\n",
      "     BatchNorm3d-460      [-1, 64, 24, 48, 160]             128\n",
      " ConvTranspose3d-461      [-1, 32, 48, 96, 320]          55,296\n",
      "     BatchNorm3d-462      [-1, 32, 48, 96, 320]              64\n",
      "       Hourglass-463  [[-1, 64, 24, 48, 160], [-1, 64, 24, 48, 160], [-1, 32, 48, 96, 320]]               0\n",
      "          Conv3d-464      [-1, 64, 24, 48, 160]          55,296\n",
      "     BatchNorm3d-465      [-1, 64, 24, 48, 160]             128\n",
      "            SiLU-466      [-1, 64, 24, 48, 160]               0\n",
      "        Conv3dBn-467      [-1, 64, 24, 48, 160]               0\n",
      "          Conv3d-468      [-1, 64, 24, 48, 160]         110,592\n",
      "     BatchNorm3d-469      [-1, 64, 24, 48, 160]             128\n",
      "        Conv3dBn-470      [-1, 64, 24, 48, 160]               0\n",
      "          Conv3d-471       [-1, 64, 12, 24, 80]         110,592\n",
      "     BatchNorm3d-472       [-1, 64, 12, 24, 80]             128\n",
      "            SiLU-473       [-1, 64, 12, 24, 80]               0\n",
      "        Conv3dBn-474       [-1, 64, 12, 24, 80]               0\n",
      "          Conv3d-475       [-1, 64, 12, 24, 80]         110,592\n",
      "     BatchNorm3d-476       [-1, 64, 12, 24, 80]             128\n",
      "            SiLU-477       [-1, 64, 12, 24, 80]               0\n",
      "        Conv3dBn-478       [-1, 64, 12, 24, 80]               0\n",
      " ConvTranspose3d-479      [-1, 64, 24, 48, 160]         110,592\n",
      "     BatchNorm3d-480      [-1, 64, 24, 48, 160]             128\n",
      " ConvTranspose3d-481      [-1, 32, 48, 96, 320]          55,296\n",
      "     BatchNorm3d-482      [-1, 32, 48, 96, 320]              64\n",
      "       Hourglass-483  [[-1, 64, 24, 48, 160], [-1, 64, 24, 48, 160], [-1, 32, 48, 96, 320]]               0\n",
      "          Conv3d-484      [-1, 64, 24, 48, 160]          55,296\n",
      "     BatchNorm3d-485      [-1, 64, 24, 48, 160]             128\n",
      "            SiLU-486      [-1, 64, 24, 48, 160]               0\n",
      "        Conv3dBn-487      [-1, 64, 24, 48, 160]               0\n",
      "          Conv3d-488      [-1, 64, 24, 48, 160]         110,592\n",
      "     BatchNorm3d-489      [-1, 64, 24, 48, 160]             128\n",
      "        Conv3dBn-490      [-1, 64, 24, 48, 160]               0\n",
      "          Conv3d-491       [-1, 64, 12, 24, 80]         110,592\n",
      "     BatchNorm3d-492       [-1, 64, 12, 24, 80]             128\n",
      "            SiLU-493       [-1, 64, 12, 24, 80]               0\n",
      "        Conv3dBn-494       [-1, 64, 12, 24, 80]               0\n",
      "          Conv3d-495       [-1, 64, 12, 24, 80]         110,592\n",
      "     BatchNorm3d-496       [-1, 64, 12, 24, 80]             128\n",
      "            SiLU-497       [-1, 64, 12, 24, 80]               0\n",
      "        Conv3dBn-498       [-1, 64, 12, 24, 80]               0\n",
      " ConvTranspose3d-499      [-1, 64, 24, 48, 160]         110,592\n",
      "     BatchNorm3d-500      [-1, 64, 24, 48, 160]             128\n",
      " ConvTranspose3d-501      [-1, 32, 48, 96, 320]          55,296\n",
      "     BatchNorm3d-502      [-1, 32, 48, 96, 320]              64\n",
      "       Hourglass-503  [[-1, 64, 24, 48, 160], [-1, 64, 24, 48, 160], [-1, 32, 48, 96, 320]]               0\n",
      "          Conv3d-504      [-1, 32, 48, 96, 320]          27,648\n",
      "     BatchNorm3d-505      [-1, 32, 48, 96, 320]              64\n",
      "            SiLU-506      [-1, 32, 48, 96, 320]               0\n",
      "        Conv3dBn-507      [-1, 32, 48, 96, 320]               0\n",
      "          Conv3d-508       [-1, 1, 48, 96, 320]             864\n",
      "          Conv3d-509      [-1, 32, 48, 96, 320]          27,648\n",
      "     BatchNorm3d-510      [-1, 32, 48, 96, 320]              64\n",
      "            SiLU-511      [-1, 32, 48, 96, 320]               0\n",
      "        Conv3dBn-512      [-1, 32, 48, 96, 320]               0\n",
      "          Conv3d-513       [-1, 1, 48, 96, 320]             864\n",
      "          Conv3d-514      [-1, 32, 48, 96, 320]          27,648\n",
      "     BatchNorm3d-515      [-1, 32, 48, 96, 320]              64\n",
      "            SiLU-516      [-1, 32, 48, 96, 320]               0\n",
      "        Conv3dBn-517      [-1, 32, 48, 96, 320]               0\n",
      "          Conv3d-518       [-1, 1, 48, 96, 320]             864\n",
      "DisparityRegression-519            [-1, 384, 1280]               0\n",
      "DisparityRegression-520            [-1, 384, 1280]               0\n",
      "DisparityRegression-521            [-1, 384, 1280]               0\n",
      "StackedHourglass-522  [[-1, 384, 1280], [-1, 384, 1280], [-1, 384, 1280]]               0\n",
      "================================================================\n",
      "Total params: 7,068,896\n",
      "Trainable params: 7,068,896\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 8294400.00\n",
      "Forward/backward pass size (MB): 17236090134651.52\n",
      "Params size (MB): 26.97\n",
      "Estimated Total Size (MB): 17236098429078.48\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = torch.device('cpu')\n",
    "model = GCNetPlus(max_disp=192, k=32).to(device)\n",
    "#out = model(rinp, linp)\n",
    "#print(out.shape)\n",
    "summary(model, [(3, 192*2, 640*2), (3, 192*2, 640*2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bae7c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def soft_argmax(voxels):\n",
    "    \"\"\"\n",
    "    Arguments: voxel patch in shape (batch_size, channel, H, W, depth)\n",
    "    Return: 3D coordinates in shape (batch_size, channel, 3)\n",
    "    \"\"\"\n",
    "    assert voxels.dim()==5\n",
    "    # alpha is here to make the largest element really big, so it\n",
    "    # would become very close to 1 after softmax\n",
    "    alpha = 1000.0 \n",
    "    N,C,H,W,D = voxels.shape\n",
    "    soft_max = nn.functional.softmax(voxels.view(N,C,-1)*alpha,dim=2)\n",
    "    soft_max = soft_max.view(voxels.shape)\n",
    "    indices_kernel = torch.arange(start=0,end=H*W*D).unsqueeze(0)\n",
    "    indices_kernel = indices_kernel.view((H,W,D))\n",
    "    conv = soft_max*indices_kernel\n",
    "    indices = conv.sum(2).sum(2).sum(2)\n",
    "    z = indices%D\n",
    "    y = (indices/D).floor()%W\n",
    "    x = (((indices/D).floor())/W).floor()%H\n",
    "    coords = torch.stack([x,y,z],dim=2)\n",
    "    return coords\n",
    "\n",
    "voxel = torch.randn(1,2,3,3,3) # (batch_size, channel, H, W, depth)\n",
    "print(voxel.shape)\n",
    "coords = soft_argmax(voxel)\n",
    "coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0dc7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
