{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "89e30a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class CostNet(nn.Module):\n",
    "\n",
    "    def __init__(self, k=32, encoder_out=128, fusion_in=512):\n",
    "        super().__init__()\n",
    "        self.cnn = CNN(k=k)\n",
    "        self.spp = SPP(conv_size = encoder_out, k=8)\n",
    "        self.fusion = nn.Sequential(\n",
    "                Conv2dBn(in_channels=fusion_in, out_channels=encoder_out, kernel_size=3, stride=1, padding=1, use_relu=True),\n",
    "                nn.Conv2d(in_channels=encoder_out, out_channels=32, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        conv2_out, conv3_out = self.cnn(inputs)           # [B, 64, 1/4H, 1/4W], [B, 128, 1/4H, 1/4W]\n",
    "\n",
    "        spp_out = self.spp(conv3_out)                    # [B, 128, 1/4H, 1/4W]\n",
    "        print(\"costnet2\", conv2_out.shape, conv3_out.shape, spp_out.shape)\n",
    "        \n",
    "        out = torch.cat([conv2_out, conv3_out, spp_out], dim=1)  # [B, 320, 1/4H, 1/4W]\n",
    "        print(\"costnet3\", out.shape)\n",
    "        out = self.fusion(out)                            # [B, 32, 1/4H, 1/4W]\n",
    "        print(\"fusion out\", out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SPP(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_size=128, k=8):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.branch1 = self.__make_branch(kernel_size=k*8, stride=k*8, insize=conv_size)\n",
    "        self.branch2 = self.__make_branch(kernel_size=k*4, stride=k*4, insize=conv_size)\n",
    "        self.branch3 = self.__make_branch(kernel_size=k*2, stride=k*2, insize=conv_size)\n",
    "        self.branch4 = self.__make_branch(kernel_size=k, stride=k, insize=conv_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out_size = inputs.size(2), inputs.size(3)\n",
    "        assert inputs.size(2)>=self.k*8 and inputs.size(3)>=self.k*8, 'inputs.size(3) size(2) must be more than {}'.format(self.k*8)\n",
    "        print(\"spp0\", inputs.shape, out_size)\n",
    "        branch1_out = F.upsample(self.branch1(inputs), size=out_size, mode='bilinear')  # [B, 32, 1/4H, 1/4W]\n",
    "        branch2_out = F.upsample(self.branch2(inputs), size=out_size, mode='bilinear')  # [B, 32, 1/4H, 1/4W]\n",
    "        branch3_out = F.upsample(self.branch3(inputs), size=out_size, mode='bilinear')  # [B, 32, 1/4H, 1/4W]\n",
    "        branch4_out = F.upsample(self.branch4(inputs), size=out_size, mode='bilinear')  # [B, 32, 1/4H, 1/4W]\n",
    "        print(\"spp1\", inputs.shape, branch1_out.shape, branch2_out.shape, branch3_out.shape, branch4_out.shape)\n",
    "        out = torch.cat([branch4_out, branch3_out, branch2_out, branch1_out], dim=1)  # [B, 128, 1/4H, 1/4W]\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def __make_branch(kernel_size, stride, insize):\n",
    "        branch = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size, stride),\n",
    "                Conv2dBn(in_channels=insize, out_channels=32, kernel_size=3, stride=1, padding=1, use_relu=True)  # kernel size maybe 1\n",
    "            )\n",
    "        return branch\n",
    "\n",
    "class Squeeze_excitation_layer(nn.Module):\n",
    "    def __init__(self, filters, se_ratio=4):\n",
    "        super(Squeeze_excitation_layer, self).__init__()\n",
    "        reduction = filters // se_ratio\n",
    "        self.se = nn.Sequential(nn.Conv2d(filters, reduction, kernel_size=1, bias=True),\n",
    "                                nn.SiLU(),\n",
    "                                nn.Conv2d(reduction, filters, kernel_size=1, bias=True),\n",
    "                                nn.Sigmoid())\n",
    "    def forward(self, inputs):\n",
    "        x = torch.mean(inputs, [2, 3], keepdim=True)\n",
    "        x = self.se(x)\n",
    "        return torch.multiply(inputs, x)\n",
    "\n",
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size = 3, padding = 1, bias=False):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, groups=nin, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "class MBConv2d_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k=1):\n",
    "        super(MBConv2d_block, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(nn.Conv2d(in_channels, out_channels * k, kernel_size=(1, 1), stride=(1, 1), padding=\"valid\", bias=False),\n",
    "               nn.BatchNorm2d(out_channels * k),\n",
    "               nn.SiLU(),\n",
    "               depthwise_separable_conv(out_channels * k, out_channels * k, kernel_size = 3, padding =\"same\", bias=False),\n",
    "               nn.BatchNorm2d(out_channels * k),\n",
    "               nn.SiLU(),\n",
    "               Squeeze_excitation_layer(filters=out_channels * k, se_ratio=4),\n",
    "               nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1), stride=(1, 1), padding=\"valid\", bias=False),\n",
    "               nn.BatchNorm2d(out_channels * k),\n",
    "               nn.Dropout(p=0.2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.net(inputs)\n",
    "        return torch.add(inputs, x)\n",
    "  \n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, k=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv0 = nn.Sequential(\n",
    "                Conv2dBn(in_channels=3, out_channels=k, kernel_size=3, stride=2, padding=1, use_relu=True),  # downsample\n",
    "                Conv2dBn(in_channels=k, out_channels=k, kernel_size=3, stride=1, padding=1, use_relu=True),\n",
    "                Conv2dBn(in_channels=k, out_channels=k, kernel_size=3, stride=1, padding=1, use_relu=True)\n",
    "            )\n",
    "        self.mbconv0 = MBConv2d_block(in_channels=k, out_channels=k, k=1)\n",
    "        self.conv1 = StackedBlocks(n_blocks=3, in_channels=k, out_channels=k*2, kernel_size=3, stride=1, padding=1, dilation=1)\n",
    "        self.mbconv1 = MBConv2d_block(in_channels=k*2, out_channels=k*2, k=1)\n",
    "        self.conv2 = StackedBlocks(n_blocks=3, in_channels=k*2, out_channels=k*4, kernel_size=3, stride=1, padding=1, dilation=1)  # downsample\n",
    "        self.mbconv2 = MBConv2d_block(in_channels=k*4, out_channels=k*4, k=1)\n",
    "        self.conv3 = StackedBlocks(n_blocks=3, in_channels=k*4, out_channels=k*8, kernel_size=3, stride=1, padding=2, dilation=2)  # dilated\n",
    "        self.mbconv3 = MBConv2d_block(in_channels=k*8, out_channels=k*8, k=1)\n",
    "        #self.conv4 = StackedBlocks(n_blocks=3, in_channels=k*4, out_channels=k*8, kernel_size=3, stride=1, padding=4, dilation=4)  # dilated\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        conv0_out = self.mbconv0(self.conv0(inputs))\n",
    "        conv1_out = self.mbconv1(self.conv1(conv0_out))  # [B, 32, 1/2H, 1/2W]\n",
    "        conv2_out = self.mbconv2(self.conv2(conv1_out))  # [B, 64, 1/4H, 1/4W]\n",
    "        conv3_out = self.mbconv3(self.conv3(conv2_out))  # [B, 128, 1/4H, 1/4W]\n",
    "        #conv4_out = self.conv4(conv3_out)  # [B, 128, 1/4H, 1/4W]\n",
    "        print(\"inp, c0 c1\", inputs.shape, conv0_out.shape, conv1_out.shape)\n",
    "        print(\"c2 c3 c4\", conv2_out.shape, conv3_out.shape)\n",
    "        return conv2_out, conv3_out\n",
    "\n",
    "\n",
    "class StackedBlocks(nn.Module):\n",
    "\n",
    "    def __init__(self, n_blocks, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super().__init__()\n",
    "\n",
    "        if stride == 1 and in_channels == out_channels:\n",
    "            downsample = False\n",
    "        else:\n",
    "            downsample = True\n",
    "        net = [ResidualBlock(in_channels, out_channels, kernel_size, stride, padding, dilation, downsample)]\n",
    "\n",
    "        for i in range(n_blocks - 1):\n",
    "            net.append(ResidualBlock(out_channels, out_channels, kernel_size, 1, padding, dilation, downsample=False))\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.net(inputs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, downsample=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "                Conv2dBn(in_channels, out_channels, kernel_size, stride, padding, dilation, use_relu=True),\n",
    "                Conv2dBn(out_channels, out_channels, kernel_size, 1, padding, dilation, use_relu=False)\n",
    "            )\n",
    "\n",
    "        self.downsample = None\n",
    "        if downsample:\n",
    "            self.downsample = Conv2dBn(in_channels, out_channels, 1, stride, use_relu=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.net(inputs)\n",
    "        if self.downsample:\n",
    "            inputs = self.downsample(inputs)\n",
    "        out = out + inputs\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Conv3dBn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, use_relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        net = [nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=False),\n",
    "               nn.BatchNorm3d(out_channels)]\n",
    "        if use_relu:\n",
    "            net.append(nn.SiLU(inplace=True))\n",
    "\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.net(inputs)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    " \n",
    "class Conv2dBn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, use_relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        net = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=False),\n",
    "               nn.BatchNorm2d(out_channels)]\n",
    "        if use_relu:\n",
    "            net.append(nn.SiLU(inplace=True))\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.net(inputs)\n",
    "        return out\n",
    "  \n",
    "class StackedHourglass(nn.Module):\n",
    "    '''\n",
    "    inputs --- [B, 64, 1/4D, 1/4H, 1/4W]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_disp):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv0 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True),\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True)\n",
    "        )\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True),\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=False)\n",
    "        )\n",
    "        self.hourglass1 = Hourglass()\n",
    "        self.hourglass2 = Hourglass()\n",
    "        self.hourglass3 = Hourglass()\n",
    "\n",
    "        self.out1 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True),\n",
    "            nn.Conv3d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        )\n",
    "        self.out2 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True),\n",
    "            nn.Conv3d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        )\n",
    "        self.out3 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True),\n",
    "            nn.Conv3d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.regression = DisparityRegression(max_disp)\n",
    "\n",
    "    def forward(self, inputs, out_size):\n",
    "\n",
    "        conv0_out = self.conv0(inputs)     # [B, 32, 1/4D, 1/4H, 1/4W]\n",
    "        conv1_out = self.conv1(conv0_out)\n",
    "        conv1_out = conv0_out + conv1_out  # [B, 32, 1/4D, 1/4H, 1/4W]\n",
    "\n",
    "        hourglass1_out1, hourglass1_out3, hourglass1_out4 = self.hourglass1(conv1_out, scale1=None, scale2=None, scale3=conv1_out)\n",
    "        hourglass2_out1, hourglass2_out3, hourglass2_out4 = self.hourglass2(hourglass1_out4, scale1=hourglass1_out3, scale2=hourglass1_out1, scale3=conv1_out)\n",
    "        hourglass3_out1, hourglass3_out3, hourglass3_out4 = self.hourglass3(hourglass2_out4, scale1=hourglass2_out3, scale2=hourglass1_out1, scale3=conv1_out)\n",
    "\n",
    "        out1 = self.out1(hourglass1_out4)  # [B, 1, 1/4D, 1/4H, 1/4W]\n",
    "        out2 = self.out2(hourglass2_out4) + out1\n",
    "        out3 = self.out3(hourglass3_out4) + out2\n",
    "\n",
    "        cost1 = F.upsample(out1, size=out_size, mode='trilinear').squeeze(dim=1)  # [B, D, H, W]\n",
    "        cost2 = F.upsample(out2, size=out_size, mode='trilinear').squeeze(dim=1)  # [B, D, H, W]\n",
    "        cost3 = F.upsample(out3, size=out_size, mode='trilinear').squeeze(dim=1)  # [B, D, H, W]\n",
    "\n",
    "        prob1 = F.softmax(-cost1, dim=1)  # [B, D, H, W]\n",
    "        prob2 = F.softmax(-cost2, dim=1)\n",
    "        prob3 = F.softmax(-cost3, dim=1)\n",
    "\n",
    "        disp1 = self.regression(prob1)\n",
    "        disp2 = self.regression(prob2)\n",
    "        disp3 = self.regression(prob3)\n",
    "\n",
    "        return disp1, disp2, disp3\n",
    "\n",
    "\n",
    "class DisparityRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, max_disp):\n",
    "        super().__init__()\n",
    "\n",
    "        self.disp_score = torch.range(0, max_disp - 1)  # [D]\n",
    "        self.disp_score = self.disp_score.unsqueeze(0).unsqueeze(2).unsqueeze(3)  # [1, D, 1, 1]\n",
    "\n",
    "    def forward(self, prob):\n",
    "        disp_score = self.disp_score.expand_as(prob).type_as(prob)  # [B, D, H, W]\n",
    "        out = torch.sum(disp_score * prob, dim=1)  # [B, H, W]\n",
    "        return out\n",
    "\n",
    "\n",
    "class Hourglass(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net1 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1, dilation=1, use_relu=True),\n",
    "            Conv3dBn(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=False)\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            Conv3dBn(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1, dilation=1, use_relu=True),\n",
    "            Conv3dBn(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, use_relu=True)\n",
    "        )\n",
    "        self.net3 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "            nn.BatchNorm3d(num_features=64)\n",
    "            # nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.net4 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "            nn.BatchNorm3d(num_features=32)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, scale1=None, scale2=None, scale3=None):\n",
    "        net1_out = self.net1(inputs)  # [B, 64, 1/8D, 1/8H, 1/8W]\n",
    "\n",
    "        if scale1 is not None:\n",
    "            net1_out = F.relu(net1_out + scale1, inplace=True)\n",
    "        else:\n",
    "            net1_out = F.relu(net1_out, inplace=True)\n",
    "\n",
    "        net2_out = self.net2(net1_out)  # [B, 64, 1/16D, 1/16H, 1/16W]\n",
    "        net3_out = self.net3(net2_out)  # [B, 64, 1/8D, 1/8H, 1/8W]\n",
    "\n",
    "        if scale2 is not None:\n",
    "            net3_out = F.relu(net3_out + scale2, inplace=True)\n",
    "        else:\n",
    "            net3_out = F.relu(net3_out + net1_out, inplace=True)\n",
    "\n",
    "        net4_out = self.net4(net3_out)\n",
    "\n",
    "        if scale3 is not None:\n",
    "            net4_out = net4_out + scale3\n",
    "\n",
    "        return net1_out, net3_out, net4_out\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "class GCNetPlus(nn.Module):\n",
    "\n",
    "    def __init__(self, max_disp, k=32, encoder_out=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cost_net = CostNet(k=k, encoder_out=encoder_out)\n",
    "        self.stackedhourglass = StackedHourglass(max_disp)\n",
    "        self.D = max_disp\n",
    "\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        st = time.time()\n",
    "        original_size = [self.D, left_img.size(2), left_img.size(3)]\n",
    "\n",
    "        left_cost = self.cost_net(left_img)  # [B, 32, 1/4H, 1/4W]\n",
    "        right_cost = self.cost_net(right_img)  # [B, 32, 1/4H, 1/4W]\n",
    "        #cost = torch.cat([left_cost, right_cost], dim=1)  # [B, 64, 1/4H, 1/4W]\n",
    "        # B, C, H, W = cost.size()\n",
    "\n",
    "        print(\"lcost, rcost\", left_cost.shape, right_cost.shape) # torch.Size([4, 32, 96, 320]) torch.Size([4, 32, 96, 320])\n",
    "        # print(left_cost[0, 0, :3, :3])\n",
    "\n",
    "        B, C, H, W = left_cost.size()\n",
    "        print(\"B, C, H, W\", B, C, H, W)\n",
    "        \n",
    "        cost_volume = torch.zeros(B, C * 2, self.D // 4, H, W).type_as(left_cost)  # [B, 64, D, 1/4H, 1/4W]\n",
    "        print(\"cost_volume1\", cost_volume.shape)\n",
    "        print(\"from start to costvolume start\", time.time() - st)\n",
    "        #\"\"\"\n",
    "        for i in range(self.D // 4):\n",
    "            if i > 0:\n",
    "                cost_volume[:, :C, i, :, i:] = left_cost[:, :, :, i:]\n",
    "                cost_volume[:, C:, i, :, i:] = right_cost[:, :, :, :-i]\n",
    "            else:\n",
    "                cost_volume[:, :C, i, :, :] = left_cost\n",
    "                cost_volume[:, C:, i, :, :] = right_cost\n",
    "       # \"\"\"\n",
    "        print(\"cost_volume2\", cost_volume.shape)\n",
    "        print(\"from start to costvolume end\", time.time() - st)\n",
    "        disp1, disp2, disp3 = self.stackedhourglass(cost_volume, out_size=original_size)\n",
    "        print(\"disp\", disp1.shape, disp2.shape, disp3.shape)\n",
    "        print(\"from start to disp end\", time.time() - st)\n",
    "        return cost_volume #disp1, disp2, disp3\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d6ce2b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 192, 320]) torch.Size([4, 3, 192, 320])\n"
     ]
    }
   ],
   "source": [
    "H= 192*1\n",
    "W=320*1\n",
    "rinp = torch.randn(4, 3, H, W)\n",
    "linp = torch.randn(4, 3, H, W)\n",
    "print(rinp.shape, linp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57900849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "inp, c0 c1 torch.Size([4, 3, 192, 320]) torch.Size([4, 32, 96, 160]) torch.Size([4, 64, 96, 160])\n",
    "c2 c3 c4 torch.Size([4, 128, 96, 160]) torch.Size([4, 256, 96, 160])\n",
    "spp0 torch.Size([4, 256, 96, 160]) (96, 160)\n",
    "spp1 torch.Size([4, 256, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160])\n",
    "costnet2 torch.Size([4, 128, 96, 160]) torch.Size([4, 256, 96, 160]) torch.Size([4, 128, 96, 160])\n",
    "costnet3 torch.Size([4, 512, 96, 160])\n",
    "fusion out torch.Size([4, 32, 96, 160])\n",
    "inp, c0 c1 torch.Size([4, 3, 192, 320]) torch.Size([4, 32, 96, 160]) torch.Size([4, 64, 96, 160])\n",
    "c2 c3 c4 torch.Size([4, 128, 96, 160]) torch.Size([4, 256, 96, 160])\n",
    "spp0 torch.Size([4, 256, 96, 160]) (96, 160)\n",
    "spp1 torch.Size([4, 256, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160])\n",
    "costnet2 torch.Size([4, 128, 96, 160]) torch.Size([4, 256, 96, 160]) torch.Size([4, 128, 96, 160])\n",
    "costnet3 torch.Size([4, 512, 96, 160])\n",
    "fusion out torch.Size([4, 32, 96, 160])\n",
    "lcost, rcost torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160])\n",
    "B, C, H, W 4 32 96 160\n",
    "cost_volume1 torch.Size([4, 64, 48, 96, 160])\n",
    "from start to costvolume start 18.042786121368408\n",
    "cost_volume2 torch.Size([4, 64, 48, 96, 160])\n",
    "from start to costvolume end 18.1296489238739\n",
    "disp torch.Size([4, 192, 320]) torch.Size([4, 192, 320]) torch.Size([4, 192, 320])\n",
    "from start to disp end 64.20352101325989\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "417b88cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fs/xcd148y15494hf9gnyplx3y00000gn/T/ipykernel_3817/3684951780.py:283: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  self.disp_score = torch.range(0, max_disp - 1)  # [D]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp, c0 c1 torch.Size([4, 3, 192, 320]) torch.Size([4, 32, 96, 160]) torch.Size([4, 64, 96, 160])\n",
      "c2 c3 c4 torch.Size([4, 128, 96, 160]) torch.Size([4, 256, 96, 160])\n",
      "spp0 torch.Size([4, 256, 96, 160]) (96, 160)\n",
      "spp1 torch.Size([4, 256, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160])\n",
      "costnet2 torch.Size([4, 128, 96, 160]) torch.Size([4, 256, 96, 160]) torch.Size([4, 128, 96, 160])\n",
      "costnet3 torch.Size([4, 512, 96, 160])\n",
      "fusion out torch.Size([4, 32, 96, 160])\n",
      "inp, c0 c1 torch.Size([4, 3, 192, 320]) torch.Size([4, 32, 96, 160]) torch.Size([4, 64, 96, 160])\n",
      "c2 c3 c4 torch.Size([4, 128, 96, 160]) torch.Size([4, 256, 96, 160])\n",
      "spp0 torch.Size([4, 256, 96, 160]) (96, 160)\n",
      "spp1 torch.Size([4, 256, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160])\n",
      "costnet2 torch.Size([4, 128, 96, 160]) torch.Size([4, 256, 96, 160]) torch.Size([4, 128, 96, 160])\n",
      "costnet3 torch.Size([4, 512, 96, 160])\n",
      "fusion out torch.Size([4, 32, 96, 160])\n",
      "lcost, rcost torch.Size([4, 32, 96, 160]) torch.Size([4, 32, 96, 160])\n",
      "B, C, H, W 4 32 96 160\n",
      "cost_volume1 torch.Size([4, 64, 48, 96, 160])\n",
      "from start to costvolume start 18.042786121368408\n",
      "cost_volume2 torch.Size([4, 64, 48, 96, 160])\n",
      "from start to costvolume end 18.1296489238739\n",
      "disp torch.Size([4, 192, 320]) torch.Size([4, 192, 320]) torch.Size([4, 192, 320])\n",
      "from start to disp end 64.20352101325989\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = torch.device('cpu')\n",
    "model = GCNetPlus(max_disp=192, k=32, encoder_out=256).to(device)\n",
    "out = model(rinp, linp)\n",
    "#print(out.shape)\n",
    "k = 1\n",
    "#summary(model, [(3, int(192*k), int(640*k)), (3, int(192*k), int(640*k))])\n",
    "# [2, 32, 96, 320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bae7c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def soft_argmax(voxels):\n",
    "    \"\"\"\n",
    "    Arguments: voxel patch in shape (batch_size, channel, H, W, depth)\n",
    "    Return: 3D coordinates in shape (batch_size, channel, 3)\n",
    "    \"\"\"\n",
    "    assert voxels.dim()==5\n",
    "    # alpha is here to make the largest element really big, so it\n",
    "    # would become very close to 1 after softmax\n",
    "    alpha = 1000.0 \n",
    "    N,C,H,W,D = voxels.shape\n",
    "    soft_max = nn.functional.softmax(voxels.view(N,C,-1)*alpha,dim=2)\n",
    "    soft_max = soft_max.view(voxels.shape)\n",
    "    indices_kernel = torch.arange(start=0,end=H*W*D).unsqueeze(0)\n",
    "    indices_kernel = indices_kernel.view((H,W,D))\n",
    "    conv = soft_max*indices_kernel\n",
    "    indices = conv.sum(2).sum(2).sum(2)\n",
    "    z = indices%D\n",
    "    y = (indices/D).floor()%W\n",
    "    x = (((indices/D).floor())/W).floor()%H\n",
    "    coords = torch.stack([x,y,z],dim=2)\n",
    "    return coords\n",
    "\n",
    "voxel = torch.randn(1,2,3,3,3) # (batch_size, channel, H, W, depth)\n",
    "print(voxel.shape)\n",
    "coords = soft_argmax(voxel)\n",
    "coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0dc7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
